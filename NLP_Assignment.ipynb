{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb05feda",
   "metadata": {},
   "source": [
    "## NLP Assignment\n",
    "\n",
    "source of dataset: SHAKESPEARE folder texts from https://www.kaggle.com/datasets/mylesoneill/classic-literature-in-ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ebbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "folder_path = \"./SHAKESPEARE\"\n",
    "text = \"\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith(\".txt\"):  # only .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "        text += content + \"\\n\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d246a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords function for any language\n",
    "def remove_stopwords(text, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    word_tokens = text.split()\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    print(f\"Language: {language}\")\n",
    "    print(\"Filtered Text:\", filtered_text)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Example usage\n",
    "ex_text = \"I hope this bootcamp is useful for you. You can share it with your friends at https://example.com\"\n",
    "remove_urls(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b10fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Stemming function\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "# Example usage\n",
    "ex_text = 'text preprocessing section in course nlp - deep learning'\n",
    "stem_words(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the resource for tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df399e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatization function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = text.split()\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Example usage\n",
    "ex_text = 'text preprocessing section in course nlp - deep learning'\n",
    "print(lemmatize_word(ex_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19094d36",
   "metadata": {},
   "source": [
    "# Text Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43542132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove emojis / non-ASCII\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove Roman numerals\n",
    "    pattern = r'\\b[MCDXLVI]+\\b'   # Roman numeral letters only, bounded by word boundaries\n",
    "    text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bc33f",
   "metadata": {},
   "source": [
    "# Text Processing for Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = nltk.sent_tokenize(text)\n",
    "\n",
    "corpus = preprocessed_sentences = [pre_process(sentence) for sentence in sentences_list]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_array = pd.DataFrame.sparse.from_spmatrix(X, columns=feature_names, index=corpus)\n",
    "\n",
    "print(\"Unique Word List: \\n\", feature_names)\n",
    "print()\n",
    "print(\"Bag of Words Matrix:\")\n",
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Product of Term Frequency & Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=terms)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_counts = np.asarray(X.sum(axis=0)).ravel()\n",
    "frequencies = dict(zip(vectorizer.get_feature_names_out(), word_counts))\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frequencies)\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vectorizer.vocabulary_)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be694b7d",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468df850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Tokenize the corpus using NLTK\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "print(\"Tokenized Corpus:\", tokenized_corpus)\n",
    "\n",
    "# Flatten the list to get all words in the corpus\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "\n",
    "# Get unique words (vocabulary)\n",
    "vocab = sorted(set(all_words))\n",
    "\n",
    "# Print vocabulary\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Reshape the list of words into a 2D array for OneHotEncoder\n",
    "word_array = np.array(all_words).reshape(-1, 1)\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=True)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(word_array)\n",
    "\n",
    "# Print the one-hot encoded data\n",
    "print(\"One-hot encoded matrix:\\n\", one_hot_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f97963",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "sentences = sentences_list\n",
    "cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0, alpha=0.03, min_alpha=0.0007, epochs=100)\n",
    "skipgram_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1, alpha=0.03, min_alpha=0.0007, epochs=100)\n",
    "\n",
    "cbow_model.train(sentences, total_examples=len(sentences), epochs=100)\n",
    "skipgram_model.train(sentences, total_examples=len(sentences), epochs=100)\n",
    "\n",
    "word_vectors_cbow = cbow_model.wv\n",
    "similarity_cbow = word_vectors_cbow.similarity('king', 'lord')\n",
    "print(f\"Similarity between 'king' and 'lord': {similarity_cbow} with CBOW\")\n",
    "\n",
    "\n",
    "word_vectors_skipgram= skipgram_model.wv\n",
    "similarity_skip = word_vectors_skipgram.similarity('king', 'lord')\n",
    "print(f\"Similarity between 'king' and 'lord': {similarity_skip} with Skip-Gram\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
